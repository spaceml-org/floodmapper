{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202abce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import pathlib\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import gc\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from dotenv import load_dotenv\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import ee\n",
    "\n",
    "import pandas as pd\n",
    "from georeader.readers import ee_query\n",
    "import folium\n",
    "import geemap.foliumap as geemap\n",
    "import shapely\n",
    "from shapely.geometry import box, Point, MultiPoint\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "from skimage.morphology import binary_dilation, disk\n",
    "\n",
    "from typing import Tuple, Callable, Any, Optional\n",
    "\n",
    "\n",
    "from ml4floods.data.worldfloods import dataset\n",
    "from ml4floods.data import utils\n",
    "from ml4floods.visualization import plot_utils\n",
    "from ml4floods.models import postprocess\n",
    "\n",
    "from db_utils import DB\n",
    "\n",
    "# Uncomment this to suppress deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n",
    "\n",
    "# Set bucket will not be requester pays\n",
    "utils.REQUESTER_PAYS_DEFAULT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de44e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "from rasterio.io import MemoryFile\n",
    "from rasterio import Affine as A\n",
    "#from rasterio.warp import reproject, Resampling\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
    "from rasterio.transform import from_origin\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10a43c",
   "metadata": {},
   "source": [
    "## Load environment and project details\n",
    "\n",
    "As with the other notebooks, we load credentials and project details from a hidden ```.env``` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (including path to credentials) from '.env' file\n",
    "env_file_path = \"../.env\"\n",
    "\n",
    "# Uncomment for alternative version for Windows (r\"\" indicates raw string)\n",
    "#env_file_path = r\"C:/Users/User/floodmapper/.env\"\n",
    "\n",
    "assert load_dotenv(dotenv_path=env_file_path) == True, \"[ERR] failed to load environment!\"\n",
    "assert \"GOOGLE_APPLICATION_CREDENTIALS\" in os.environ, \"[ERR] missing $GOOGLE_APPLICATION_CREDENTIAL!\"\n",
    "assert \"GS_USER_PROJECT\" in os.environ, \"[ERR] missing $GS_USER_PROJECT!\"\n",
    "key_file_path = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]\n",
    "assert os.path.exists(key_file_path), f\"[ERR] Google credential key file does not exist: \\n{key_file_path} \"\n",
    "assert \"ML4FLOODS_BASE_DIR\" in os.environ, \"[ERR] missing $ML4FLOODS_BASE_DIR!\"\n",
    "base_path = os.environ[\"ML4FLOODS_BASE_DIR\"]\n",
    "assert os.path.exists(base_path), f\"[ERR] base path does not exist: \\n{base_path} \"\n",
    "bucket_name = os.environ[\"BUCKET_URI\"]\n",
    "assert bucket_name is not None and bucket_name != \"\", f\"Bucket name not defined {bucket_name}\"\n",
    "\n",
    "print(\"[INFO] Successfully loaded FloodMapper environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc803fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database (point to the .env file for credentials)\n",
    "db_conn = DB(env_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the Google Earth Engine connection.\n",
    "# Follow instructions on login prompt, if required.\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfd378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL: All work is conducted under a unique session name\n",
    "session_name = \"boulia_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7ce41",
   "metadata": {},
   "source": [
    "## Retrieve the session parameters from the database\n",
    "\n",
    "Edit the following cell to set the session name and retrieve the flood map parameters, including mapping grid and affected LGAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b6091",
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = ZoneInfo(\"UTC\")\n",
    "midnight = datetime.min.time()\n",
    "\n",
    "# Query the floodmapping parameters from the DB\n",
    "query = (f\"SELECT flood_date_start, flood_date_end, \"\n",
    "         f\"ref_date_start, ref_date_end, bucket_uri \"\n",
    "         f\"FROM session_info \"\n",
    "         f\"WHERE session = %s;\")\n",
    "data = (session_name,)\n",
    "session_df = db_conn.run_query(query, data, fetch=True)\n",
    "flood_start_date = session_df.iloc[0][\"flood_date_start\"]\n",
    "flood_start_date = datetime.combine(flood_start_date, midnight).replace(tzinfo=tz)\n",
    "flood_end_date = session_df.iloc[0][\"flood_date_end\"]\n",
    "flood_end_date = datetime.combine(flood_end_date, midnight).replace(tzinfo=tz)\n",
    "ref_start_date = session_df.iloc[0][\"ref_date_start\"]\n",
    "ref_start_date = datetime.combine(ref_start_date, midnight).replace(tzinfo=tz)\n",
    "ref_end_date = session_df.iloc[0][\"ref_date_end\"]\n",
    "ref_end_date = datetime.combine(ref_end_date, midnight).replace(tzinfo=tz)\n",
    "bucket_uri = session_df.iloc[0][\"bucket_uri\"]\n",
    "\n",
    "# Query the selected grid positions and LGAs\n",
    "query = (f\"SELECT sp.patch_name, ST_AsText(gr.geometry), gr.lga_name22 \"\n",
    "         f\"FROM session_patches sp \"\n",
    "         f\"INNER JOIN grid_loc gr \"\n",
    "         f\"ON sp.patch_name = gr.patch_name \"\n",
    "         f\"WHERE sp.session = %s ;\")\n",
    "data = (session_name,)\n",
    "grid_sel_df = db_conn.run_query(query, data, fetch=True)\n",
    "\n",
    "# Format the results into a correct GeoDataFrame\n",
    "grid_sel_df['geometry'] = gpd.GeoSeries.from_wkt(grid_sel_df['st_astext'])\n",
    "grid_sel_df.drop(['st_astext'], axis=1, inplace = True)\n",
    "grid_sel_gdf = gpd.GeoDataFrame(grid_sel_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "grid_sel_gdf.drop_duplicates(subset=[\"patch_name\"], inplace=True)\n",
    "print(f\"[INFO] {len(grid_sel_gdf)} grid patches selected.\")\n",
    "\n",
    "# Query the affected LGA shapes\n",
    "lgas_sel_lst = grid_sel_df.lga_name22.unique().tolist()\n",
    "query = (f\"SELECT DISTINCT lga_name22, ST_AsText(geometry_col) \"\n",
    "         f\"FROM lgas_info \"\n",
    "         f\"WHERE lga_name22 IN %s ;\")\n",
    "data = (tuple(lgas_sel_lst),)\n",
    "lgas_sel_df = db_conn.run_query(query, data, fetch=True)\n",
    "\n",
    "# Format the results into a correct GeoDataFrame\n",
    "lgas_sel_df['geometry'] = gpd.GeoSeries.from_wkt(lgas_sel_df['st_astext'])\n",
    "lgas_sel_df.drop(['st_astext'], axis=1, inplace = True)\n",
    "lgas_sel_gdf = gpd.GeoDataFrame(lgas_sel_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "print(f\"[INFO] {len(lgas_sel_gdf)} LGAs affected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47130eea",
   "metadata": {},
   "source": [
    "## Problem patches\n",
    "\n",
    "We will use the following patches during development and testing:\n",
    "\n",
    "* GRID22075\n",
    "* GRID22244\n",
    "* GRID21909\n",
    "* GRID21910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2566852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pactch to be processed\n",
    "#patch_name = \"GRID22075\"\n",
    "#patch_name = \"GRID21910\"\n",
    "patch_name = \"GRID21909\"\n",
    "#patch_name = \"GRID22412\"\n",
    "\n",
    "# Select the list of flood-maps during the time range\n",
    "# Order by satellite so as S2 are first\n",
    "query = (f\"SELECT DISTINCT satellite, date, data_path \"\n",
    "         f\"FROM inference \"\n",
    "         f\"WHERE patch_name = %s \"\n",
    "         f\"AND date >= %s \"\n",
    "         f\"AND date <= %s \"\n",
    "         f\"AND mode = %s \"\n",
    "         f\"AND status = %s \"\n",
    "         f\"ORDER BY satellite DESC, date ASC\")\n",
    "data = [patch_name, flood_start_date, flood_end_date, 'pred', 1]\n",
    "geojsons_df = db_conn.run_query(query, data, fetch=True)\n",
    "num_files = len(geojsons_df)\n",
    "print(f\"[INFO] Found {num_files} flood maps in the database.\")\n",
    "geojsons_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b25ab8c",
   "metadata": {},
   "source": [
    "## Final function for performing temporal aggregation\n",
    "\n",
    "The function get_floodmap_post() expects a list of paths to vector GeoJSONs. We will build a function that expects a list of paths to raster GeoJSONs and perform the vectorisation step after."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1681436e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the list of GeoJSONs from the dataframe.\n",
    "# Sort so that Sentinel-2 is the first file in the list\n",
    "geojsons_lst = geojsons_df[\"data_path\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd8054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the vectorise function\n",
    "def vectorize_outputv1(prediction: np.ndarray,\n",
    "                       crs: Any,\n",
    "                       transform: rasterio.Affine,\n",
    "                       border:int=2) -> Optional[gpd.GeoDataFrame]:\n",
    "    \"\"\"\n",
    "    Convert a raster mask into a vectorised GeoDataFrame.\n",
    "\n",
    "    Args:\n",
    "        prediction: (H, W) array with 4 posible values [0: \"invalid\",\n",
    "                    2: \"water\", 3: \"cloud\", 4: \"flood_trace\"]\n",
    "        crs:        coordinate reference system\n",
    "        transform:  transformation matrix\n",
    "        border:     set border pixels to zero\n",
    "\n",
    "    Returns:\n",
    "        GeoDataFrame with vectorised masks\n",
    "    \"\"\"\n",
    "    data_out = []\n",
    "    start = 0\n",
    "    class_name = {0: \"area_imaged\", 2: \"water\", 3: \"cloud\", 4: \"flood_trace\"}\n",
    "    # Dilate invalid mask\n",
    "    invalid_mask = binary_dilation(prediction == 0, disk(3)).astype(bool)\n",
    "\n",
    "    # Set borders to zero to avoid border effects when vectorizing\n",
    "    prediction[:border,:] = 0\n",
    "    prediction[:, :border] = 0\n",
    "    prediction[-border:, :] = 0\n",
    "    prediction[:, -border:] = 0\n",
    "    prediction[invalid_mask] = 0\n",
    "\n",
    "    # Loop through the mask classes\n",
    "    for c, cn in class_name.items():\n",
    "        if c == 0:\n",
    "            # To remove stripes in area imaged\n",
    "            mask = prediction != c\n",
    "        else:\n",
    "            mask = prediction == c\n",
    "\n",
    "        geoms_polygons = \\\n",
    "            postprocess.get_water_polygons(mask, transform=transform)\n",
    "        if len(geoms_polygons) > 0:\n",
    "            data_out.append(gpd.GeoDataFrame(\n",
    "                {\"geometry\": geoms_polygons,\n",
    "                 \"id\": np.arange(start, start + len(geoms_polygons)),\n",
    "                 \"class\": cn},\n",
    "                crs=crs))\n",
    "        start += len(geoms_polygons)\n",
    "\n",
    "    if len(data_out) == 1:\n",
    "        return data_out[0]\n",
    "    elif len(data_out) > 1:\n",
    "        return pd.concat(data_out, ignore_index=True)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b31910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_maximal_floodraster(geojsons_lst, verbose=False):\n",
    "    \"\"\"\n",
    "    Calculate the maximal flood extent from the integer-based \n",
    "    raster flood masks.  \n",
    "    \"\"\"\n",
    "\n",
    "    is_first = True\n",
    "    geojsons_lst.sort(reverse=True) # Sort so that S2 is first\n",
    "    for filename in geojsons_lst:\n",
    "        if verbose:\n",
    "            sat_file = \"_\".join(pathlib.Path(filename).parts[-2:])\n",
    "            print(f\"[INFO] temporal merge '{sat_file}'\")\n",
    "        with rasterio.open(filename) as src:\n",
    "            if is_first:\n",
    "                is_first = False\n",
    "                # Read the header and raster array\n",
    "                profile = src.profile.copy()\n",
    "                band1 = src.read(1)\n",
    "                # Record the target CRS and dimensions\n",
    "                dst_crs = src.crs\n",
    "                dst_width = src.width\n",
    "                dst_height = src.height\n",
    "                dst_bounds = src.bounds\n",
    "                # Create the initial masks\n",
    "                valid = band1 != 0\n",
    "                water = band1 == 2\n",
    "                cloud = band1 == 3\n",
    "                flood_trace = band1 == 4\n",
    "            else:\n",
    "                # Calculate the output transformation matrix\n",
    "                dst_transform, dst_width, dst_height = \\\n",
    "                calculate_default_transform(\n",
    "                    src.crs,\n",
    "                    dst_crs,\n",
    "                    dst_width,\n",
    "                    dst_height,\n",
    "                    *dst_bounds)\n",
    "                # Build a new header\n",
    "                dst_kwargs = src.meta.copy()\n",
    "                dst_kwargs.update({\n",
    "                    'crs': dst_crs,\n",
    "                    'transform': dst_transform,\n",
    "                    'width': dst_width,\n",
    "                    'height': dst_height,\n",
    "                    'nodata': 0\n",
    "                })\n",
    "                # Perform operations in memory\n",
    "                with MemoryFile() as memfile:\n",
    "                    # Reproject band 1 to a memory file\n",
    "                    with memfile.open(**dst_kwargs) as dst:\n",
    "                        reproject(\n",
    "                            source=rasterio.band(src, 1),\n",
    "                            destination=rasterio.band(dst, 1),\n",
    "                            src_transform=src.transform,\n",
    "                            src_crs=src.crs,\n",
    "                            dst_transform=dst_transform,\n",
    "                            dst_crs=dst_crs,\n",
    "                            resampling=Resampling.nearest)\n",
    "                            #resampling=Resampling.bilinear)\n",
    "                    # Accumulate the masks onto the final arrays\n",
    "                    with memfile.open() as mch:\n",
    "                        band1 = mch.read(1)\n",
    "                        # Water always accumulates into a maximum extent\n",
    "                        water += (band1 == 2)\n",
    "                        # Flood_trace accumulates, except where it converts to water\n",
    "                        flood_trace += (band1 == 4)\n",
    "                        flood_trace = np.where(water, False, flood_trace)\n",
    "                        # Cloud accumulates, but is nulified by water, flood_trace and land.\n",
    "                        # Aim is to only have cloud masks where no data exists because of clouds.\n",
    "                        cloud += (band1 == 3)\n",
    "                        cloud = np.where(water, False, cloud)\n",
    "                        cloud = np.where(flood_trace, False, cloud)\n",
    "                        land = (band1 != 0) & (band1 != 2) & (band1 != 3) & (band1 != 4)\n",
    "                        cloud = np.where(land, False, cloud)\n",
    "                        # Valid data accumulates as a maximum extent\n",
    "                        valid += (band1 != 0)\n",
    "            \n",
    "        plt.imshow(water)\n",
    "        plt.show()\n",
    "        \n",
    "    # Assemble the final array\n",
    "    out_raster = np.zeros_like(band1)\n",
    "    out_raster = np.where(valid, 1, out_raster)\n",
    "    out_raster = np.where(cloud, 3, out_raster)\n",
    "    out_raster = np.where(flood_trace, 4, out_raster)\n",
    "    out_raster = np.where(water, 2, out_raster)\n",
    "    \n",
    "    # Vectorise the output\n",
    "    floodmap = vectorize_outputv1(out_raster, profile['crs'], profile['transform'])\n",
    "\n",
    "    return out_raster, profile, floodmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ccb09",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Call the maximal map calculator and plot\n",
    "raster, profile, floodmap = calc_maximal_floodraster(geojsons_lst, True)\n",
    "plt.imshow(raster)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297a24ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the floodmap\n",
    "plot_utils.plot_floodmap(floodmap) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38af6273",
   "metadata": {},
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff363143",
   "metadata": {},
   "source": [
    "https://gis.stackexchange.com/questions/279953/numpy-array-to-gtiff-using-rasterio-without-source-raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16805926",
   "metadata": {},
   "outputs": [],
   "source": [
    "raster.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "06a1ff19",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (floodmapper)",
   "language": "python",
   "name": "floodmapper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
