{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202abce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "import pathlib\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import gc\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "from dotenv import load_dotenv\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors\n",
    "import ee\n",
    "from georeader.readers import ee_query\n",
    "import folium\n",
    "import geemap.foliumap as geemap\n",
    "import shapely\n",
    "from shapely.geometry import box, Point, MultiPoint\n",
    "from shapely.ops import nearest_points\n",
    "\n",
    "\n",
    "from ml4floods.data.worldfloods import dataset\n",
    "from ml4floods.data import utils\n",
    "from ml4floods.visualization import plot_utils\n",
    "\n",
    "from db_utils import DB\n",
    "\n",
    "# Uncomment this to suppress deprecation warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "from shapely.errors import ShapelyDeprecationWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ShapelyDeprecationWarning) \n",
    "\n",
    "# Set bucket will not be requester pays\n",
    "utils.REQUESTER_PAYS_DEFAULT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de44e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasterio\n",
    "#from rasterio.enums import Resampling\n",
    "from rasterio import Affine as A\n",
    "from rasterio.warp import reproject, Resampling\n",
    "from rasterio.warp import calculate_default_transform, reproject, Resampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c10a43c",
   "metadata": {},
   "source": [
    "## Load environment and project details\n",
    "\n",
    "As with the other notebooks, we load credentials and project details from a hidden ```.env``` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddff7ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (including path to credentials) from '.env' file\n",
    "env_file_path = \"../.env\"\n",
    "\n",
    "# Uncomment for alternative version for Windows (r\"\" indicates raw string)\n",
    "#env_file_path = r\"C:/Users/User/floodmapper/.env\"\n",
    "\n",
    "assert load_dotenv(dotenv_path=env_file_path) == True, \"[ERR] failed to load environment!\"\n",
    "assert \"GOOGLE_APPLICATION_CREDENTIALS\" in os.environ, \"[ERR] missing $GOOGLE_APPLICATION_CREDENTIAL!\"\n",
    "assert \"GS_USER_PROJECT\" in os.environ, \"[ERR] missing $GS_USER_PROJECT!\"\n",
    "key_file_path = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]\n",
    "assert os.path.exists(key_file_path), f\"[ERR] Google credential key file does not exist: \\n{key_file_path} \"\n",
    "assert \"ML4FLOODS_BASE_DIR\" in os.environ, \"[ERR] missing $ML4FLOODS_BASE_DIR!\"\n",
    "base_path = os.environ[\"ML4FLOODS_BASE_DIR\"]\n",
    "assert os.path.exists(base_path), f\"[ERR] base path does not exist: \\n{base_path} \"\n",
    "bucket_name = os.environ[\"BUCKET_URI\"]\n",
    "assert bucket_name is not None and bucket_name != \"\", f\"Bucket name not defined {bucket_name}\"\n",
    "\n",
    "print(\"[INFO] Successfully loaded FloodMapper environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc803fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database (point to the .env file for credentials)\n",
    "db_conn = DB(env_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df4e3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the Google Earth Engine connection.\n",
    "# Follow instructions on login prompt, if required.\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcfd378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EDIT THIS CELL: All work is conducted under a unique session name\n",
    "session_name = \"boulia_test\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d7ce41",
   "metadata": {},
   "source": [
    "## Retrieve the session parameters from the database\n",
    "\n",
    "Edit the following cell to set the session name and retrieve the flood map parameters, including mapping grid and affected LGAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b6091",
   "metadata": {},
   "outputs": [],
   "source": [
    "tz = ZoneInfo(\"UTC\")\n",
    "midnight = datetime.min.time()\n",
    "\n",
    "# Query the floodmapping parameters from the DB\n",
    "query = (f\"SELECT flood_date_start, flood_date_end, \"\n",
    "         f\"ref_date_start, ref_date_end, bucket_uri \"\n",
    "         f\"FROM session_info \"\n",
    "         f\"WHERE session = %s;\")\n",
    "data = (session_name,)\n",
    "session_df = db_conn.run_query(query, data, fetch=True)\n",
    "flood_start_date = session_df.iloc[0][\"flood_date_start\"]\n",
    "flood_start_date = datetime.combine(flood_start_date, midnight).replace(tzinfo=tz)\n",
    "flood_end_date = session_df.iloc[0][\"flood_date_end\"]\n",
    "flood_end_date = datetime.combine(flood_end_date, midnight).replace(tzinfo=tz)\n",
    "ref_start_date = session_df.iloc[0][\"ref_date_start\"]\n",
    "ref_start_date = datetime.combine(ref_start_date, midnight).replace(tzinfo=tz)\n",
    "ref_end_date = session_df.iloc[0][\"ref_date_end\"]\n",
    "ref_end_date = datetime.combine(ref_end_date, midnight).replace(tzinfo=tz)\n",
    "bucket_uri = session_df.iloc[0][\"bucket_uri\"]\n",
    "\n",
    "# Query the selected grid positions and LGAs\n",
    "query = (f\"SELECT sp.patch_name, ST_AsText(gr.geometry), gr.lga_name22 \"\n",
    "         f\"FROM session_patches sp \"\n",
    "         f\"INNER JOIN grid_loc gr \"\n",
    "         f\"ON sp.patch_name = gr.patch_name \"\n",
    "         f\"WHERE sp.session = %s ;\")\n",
    "data = (session_name,)\n",
    "grid_sel_df = db_conn.run_query(query, data, fetch=True)\n",
    "\n",
    "# Format the results into a correct GeoDataFrame\n",
    "grid_sel_df['geometry'] = gpd.GeoSeries.from_wkt(grid_sel_df['st_astext'])\n",
    "grid_sel_df.drop(['st_astext'], axis=1, inplace = True)\n",
    "grid_sel_gdf = gpd.GeoDataFrame(grid_sel_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "grid_sel_gdf.drop_duplicates(subset=[\"patch_name\"], inplace=True)\n",
    "print(f\"[INFO] {len(grid_sel_gdf)} grid patches selected.\")\n",
    "\n",
    "# Query the affected LGA shapes\n",
    "lgas_sel_lst = grid_sel_df.lga_name22.unique().tolist()\n",
    "query = (f\"SELECT DISTINCT lga_name22, ST_AsText(geometry_col) \"\n",
    "         f\"FROM lgas_info \"\n",
    "         f\"WHERE lga_name22 IN %s ;\")\n",
    "data = (tuple(lgas_sel_lst),)\n",
    "lgas_sel_df = db_conn.run_query(query, data, fetch=True)\n",
    "\n",
    "# Format the results into a correct GeoDataFrame\n",
    "lgas_sel_df['geometry'] = gpd.GeoSeries.from_wkt(lgas_sel_df['st_astext'])\n",
    "lgas_sel_df.drop(['st_astext'], axis=1, inplace = True)\n",
    "lgas_sel_gdf = gpd.GeoDataFrame(lgas_sel_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "print(f\"[INFO] {len(lgas_sel_gdf)} LGAs affected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47130eea",
   "metadata": {},
   "source": [
    "## Problem patches\n",
    "\n",
    "We will use the following patches during development and testing:\n",
    "\n",
    "* GRID22075\n",
    "* GRID22244\n",
    "* GRID21909\n",
    "* GRID21910"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2566852",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the pactch to be displayed\n",
    "patch_name = \"GRID22075\"\n",
    "\n",
    "# Select the list of flood-maps during the time range\n",
    "# Order by satellite so as S2 are first\n",
    "query = (f\"SELECT DISTINCT satellite, date, data_path \"\n",
    "         f\"FROM inference \"\n",
    "         f\"WHERE patch_name = %s \"\n",
    "         f\"AND date >= %s \"\n",
    "         f\"AND date <= %s \"\n",
    "         f\"AND mode = %s \"\n",
    "         f\"AND status = %s \"\n",
    "         f\"ORDER BY satellite DESC, date ASC\")\n",
    "data = [patch_name, flood_start_date, flood_end_date, 'vect', 1]\n",
    "geojsons_df = db_conn.run_query(query, data, fetch=True)\n",
    "num_files = len(geojsons_df)\n",
    "print(f\"[INFO] Found {num_files} flood maps in the database.\")\n",
    "geojsons_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6997ce2",
   "metadata": {},
   "source": [
    "Construct the path to the integer masks. The default ```data_path``` is the vector mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b50cc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip the \"_var\" from the filepath to create the address for the integer mask\n",
    "geojsons_df['mask_path'] = geojsons_df['data_path'].str.replace('_vec','')\n",
    "geojsons_df['mask_path'] = geojsons_df['mask_path'].str.replace('geojson','tif')\n",
    "for index, row in geojsons_df.iterrows():\n",
    "    print(index, row[\"mask_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3771f4",
   "metadata": {},
   "source": [
    "## Co-registering images to a common projection and sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb95c35",
   "metadata": {},
   "source": [
    "Use the 1st image as a template to reproject the other images in the time-series.\n",
    "\n",
    "See:\n",
    "https://pygis.io/docs/e_raster_resample.html\n",
    "https://rasterio.readthedocs.io/en/latest/topics/reproject.html\n",
    "https://rasterio.readthedocs.io/en/latest/topics/memory-files.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef10d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_first = True\n",
    "reproj_file_list = []\n",
    "for index, row in geojsons_df.iterrows():\n",
    "    filename = row[\"mask_path\"]\n",
    "    print(f\"{filename}\")\n",
    "    outfilename = \"_\".join(pathlib.Path(filename).parts[-2:])\n",
    "    print(f\"[INFO] Outfile: '{outfilename}'.\")\n",
    "    \n",
    "    if is_first:\n",
    "        with rasterio.open(filename) as match:\n",
    "            dst_crs = match.crs\n",
    "            dst_width = match.width\n",
    "            dst_height = match.height\n",
    "            dst_bounds = match.bounds\n",
    "        is_first = False\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        src = rasterio.open(filename)\n",
    "        \n",
    "        # Calculate the output transform matrix\n",
    "        dst_transform, dst_width, dst_height = calculate_default_transform(\n",
    "            src.crs,\n",
    "            dst_crs,\n",
    "            dst_width,\n",
    "            dst_height,\n",
    "            *dst_bounds)\n",
    "        \n",
    "        # Build a new header\n",
    "        dst_kwargs = src.meta.copy()\n",
    "        dst_kwargs.update({\n",
    "            'crs': dst_crs,\n",
    "            'transform': dst_transform,\n",
    "            'width': dst_width,\n",
    "            'height': dst_height,\n",
    "            #'nodata': 0\n",
    "        })\n",
    "        print(f\"[INFO] Coregistered to shape: [{dst_height}, {dst_width}],\\n\")\n",
    "        \n",
    "        # Open a destination file and reproject\n",
    "        with rasterio.open(outfilename, \"w\", **dst_kwargs) as dst:\n",
    "            # Iterate through bands and write using reproject function\n",
    "            for i in range(1, src.count + 1):\n",
    "                reproject(\n",
    "                    source=rasterio.band(src, i),\n",
    "                    destination=rasterio.band(dst, i),\n",
    "                    src_transform=src.transform,\n",
    "                    src_crs=src.crs,\n",
    "                    dst_transform=dst_transform,\n",
    "                    dst_crs=dst_crs,\n",
    "                    resampling=Resampling.nearest)\n",
    "                    #resampling=Resampling.bilinear)\n",
    "        reproj_file_list.append(outfilename)\n",
    "        src.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20276b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reproj_file_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb66afa3",
   "metadata": {},
   "source": [
    "## Perform the temporal merge for maximal extent\n",
    "\n",
    "Mask values are:\n",
    "* 0 = area_imaged (invalid)\n",
    "* 1 = land\n",
    "* 2 = water\n",
    "* 3 = cloud\n",
    "* 4 = flood_trace\n",
    "\n",
    "We want to build the final mask in accumulation mode, so the procedure is:\n",
    "\n",
    "\n",
    "* Accumulate all water AND water-trace pixels onto the base map.\n",
    "* Accumulate cloud pixels IF they fall on NOT-WATER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fba5630",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "is_first = True\n",
    "for filename in reproj_file_list:\n",
    "    with rasterio.open(filename) as src:\n",
    "        profile = src.profile.copy()\n",
    "        band1 = src.read(1)\n",
    "        if is_first:\n",
    "            is_first = False\n",
    "            valid = band1 != 0\n",
    "            water = band1 == 2\n",
    "            flood_trace = band1 == 4\n",
    "            cloud = band1 == 3\n",
    "        else:\n",
    "            # Water always accumulates into a maximum extent\n",
    "            water += (band1 == 2)\n",
    "            # Flood_trace accumulates, except where it converts to water\n",
    "            flood_trace += (band1 == 4)\n",
    "            flood_trace = np.where(water, False, flood_trace)\n",
    "            # Cloud accumulates, but is nullified by water, flood_trace and land.\n",
    "            # Aim is to only have cloud masks where no data exists because of clouds.\n",
    "            cloud += (band1 == 3)\n",
    "            cloud = np.where(water, False, cloud)\n",
    "            cloud = np.where(flood_trace, False, cloud)\n",
    "            land = (band1 != 0) & (band1 != 2) & (band1 != 3) & (band1 != 4)\n",
    "            cloud = np.where(land, False, cloud)\n",
    "            # Valid data accumulates as a maximum extent\n",
    "            valid += (band1 != 0)\n",
    "        \n",
    "        plt.imshow(water)\n",
    "        plt.show()\n",
    "        \n",
    "# Assemble the final array\n",
    "out_raster = np.zeros_like(band1)\n",
    "out_raster = np.where(valid, 1, out_raster)\n",
    "out_raster = np.where(cloud, 3, out_raster)\n",
    "out_raster = np.where(flood_trace, 4, out_raster)\n",
    "out_raster = np.where(water, 2, out_raster)\n",
    "plt.imshow(out_raster)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d985d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the accumulated dataset to a file\n",
    "with rasterio.open(\"FINAL_GRID22075.tif\", \n",
    "                  mode=\"w\", \n",
    "                  **profile,) as update_dataset:\n",
    "    update_dataset.write(out_raster, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (floodmapper)",
   "language": "python",
   "name": "floodmapper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
