{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d957ff8",
   "metadata": {},
   "source": [
    "# Extract EMS Data on EMSR586: Sydney Floods\n",
    "\n",
    "This notebook shows how to acquire and parse flood metadata, and initial floodmaps, from the Copernicus Emergency Management Service web page. At the end of the notebook we will have selected the local government areas affected by flooding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdbf943",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The Copernicus EMS Activation Mappings for individual events given by unique EMSR codes may be accessed [here](https://emergency.copernicus.eu/mapping/list-of-activations-rapid). Available resources include ZIP files contain multiple products (e.g., shapefiles) for an event.\n",
    "\n",
    "**Update 2023: Recent website changes mean that the tables containing the EMSR information are displayed via an *iframe*.** \n",
    "\n",
    "The direct URL for these tables is [https://poc-d8.lolandese.site/search-activations1](https://poc-d8.lolandese.site/search-activations1)\n",
    "\n",
    "The EMSR586 event covers flooding in NSW during early July 2022.\n",
    "https://emergency.copernicus.eu/mapping/list-of-components/EMSR586"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d7b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Python modules\n",
    "import sys\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely.geometry import mapping\n",
    "from shapely.geometry import box\n",
    "import ee\n",
    "#import geemap.folium as geemap\n",
    "import geemap.foliumap as geemap\n",
    "import folium\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', '', RuntimeWarning)\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from ml4floods.data import ee_download, create_gt, utils\n",
    "from ml4floods.data.copernicusEMS import activations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd60d9",
   "metadata": {},
   "source": [
    "## Load environment and project details\n",
    "\n",
    "As with the other notebooks, we load credentials and project details from a hidden ```.env``` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c23c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (including path to credentials) from '.env' file\n",
    "env_file_path = \"../.env\"\n",
    "\n",
    "# Uncomment for alternative version for Windows (r\"\" indicates raw string)\n",
    "#env_file_path = r\"C:/Users/User/floodmapper/.env\"\n",
    "\n",
    "assert load_dotenv(dotenv_path=env_file_path) == True, \"[ERR] failed to load environment!\"\n",
    "assert \"GOOGLE_APPLICATION_CREDENTIALS\" in os.environ, \"[ERR] missing $GOOGLE_APPLICATION_CREDENTIAL!\"\n",
    "assert \"GS_USER_PROJECT\" in os.environ, \"[ERR] missing $GS_USER_PROJECT!\"\n",
    "key_file_path = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]\n",
    "assert os.path.exists(key_file_path), f\"[ERR] Google credential key file does not exist: \\n{key_file_path} \"\n",
    "assert \"ML4FLOODS_BASE_DIR\" in os.environ, \"[ERR] missing $ML4FLOODS_BASE_DIR!\"\n",
    "base_path = os.environ[\"ML4FLOODS_BASE_DIR\"]\n",
    "assert os.path.exists(base_path), f\"[ERR] base path does not exist: \\n{base_path} \"\n",
    "bucket_name = os.environ[\"BUCKET_URI\"]\n",
    "assert bucket_name is not None and bucket_name != \"\", f\"Bucket name not defined {bucket_name}\"\n",
    "print(\"[INFO] Successfully loaded FloodMapper environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f21a0c",
   "metadata": {},
   "source": [
    "**Set the details of the event and mapping session here.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543f2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Official EMSR Code on Copernicus website\n",
    "emsr_code = \"EMSR586\"\n",
    "\n",
    "# Flooding date range (UTC)\n",
    "# May need to start day or two earlier\n",
    "flood_start_date = \"2022-07-01\"\n",
    "flood_end_date = \"2022-07-24\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c50501",
   "metadata": {},
   "source": [
    "## Download EMS Rapid Activation Data\n",
    "\n",
    "First let's take a look at the recent Copernicus activations using the ML4Floods 'activations' module, which provides methods to query the EMS, download event data and parse information within.\n",
    "\n",
    "**Note: ML4Floods code updated in late December 2023 with new EMSR scraping code.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8031e200",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fetch a table of activations events after a specified date\n",
    "table_activations_ems = activations.table_floods_ems(event_start_date=flood_start_date)\n",
    "table_activations_ems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9b78bd",
   "metadata": {},
   "source": [
    "We can see that information on EMSR587 is available, starting on 2022-07-03. Next we retrieve the URLs pointing to the activation products in ZIP files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f677a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retreive the URLs of available ZIP files for EMSR586\n",
    "zip_files_activation_url_list = activations.fetch_zip_file_urls(emsr_code)\n",
    "print(\"There are {:d} ZIP files available.\".format(len(zip_files_activation_url_list)))\n",
    "zip_files_activation_url_list.sort()\n",
    "zip_files_activation_url_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517a55ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the local folder to save into and create if necessary.\n",
    "# By convention, we use the folder \"<base_path>/flood-activations/<emsr_code>\".\n",
    "folder_out = os.path.join(base_path, \"flood-activations\", emsr_code, \"ems_products\").replace(\"\\\\\", \"/\")\n",
    "print(\"[INFO] Local output folder:\\n\", folder_out)\n",
    "os.makedirs(folder_out, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ac05a",
   "metadata": {},
   "source": [
    "Now download the files (if necessary) and unzip into folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5733a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the available files, download if necessary, and unzip\n",
    "zip_files_activations = []\n",
    "unzip_files_activations = []\n",
    "print(\"[INFO] downloading {:d} files\".format(len(zip_files_activation_url_list)))\n",
    "for url_str in tqdm(zip_files_activation_url_list):\n",
    "    zip_file_name = os.path.split(url_str)[-1]\n",
    "    local_zip_file = os.path.join(folder_out, zip_file_name).replace(\"\\\\\", \"/\")\n",
    "    if not os.path.exists(local_zip_file):\n",
    "        # Perform the download\n",
    "        local_zip_file = activations.download_vector_cems(url_str, folder_out=folder_out)\n",
    "    zip_files_activations.append(local_zip_file)\n",
    "print(\"[INFO] unzipping {:d} files\".format(len(zip_files_activation_url_list)))\n",
    "for local_zip_file in tqdm(zip_files_activations):\n",
    "    unzipped_file = activations.unzip_copernicus_ems(local_zip_file, folder_out=folder_out)\n",
    "    unzip_files_activations.append(unzipped_file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8209255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the files have been downloaded and unzipped\n",
    "cmd = f\"tree -L 1 {folder_out}\"\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c51bb3",
   "metadata": {},
   "source": [
    "Each folder contains shapefiles with useful information on:\n",
    "\n",
    "* facilitiesL = linear features (power lines)\n",
    "* facilitiesA = facilities (parks, golf courses, plants)\n",
    "* builtUpA = build up areas (urban areas)\n",
    "* transportationL = transportations lines (e.g., roads and tracks)\n",
    "* transportationA = transportation areas (e.g., airports)\n",
    "* physiographyL = physiography lines (e.g., height contours)\n",
    "* naturalLandUseA = land use areas\n",
    "* imageFootprintA = source image footprint\n",
    "\n",
    "However, the information we care about is in these files:\n",
    "\n",
    "* observedEventA = observed event areas (polygons)\n",
    "* hydrographyL = linear water bodies (permanent streams)\n",
    "* hydrographyA = wide water bodies (permanent lakes, rivers)\n",
    "* areaOfInterest = polygon around AoI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b264ea",
   "metadata": {},
   "source": [
    "## Building EMSR Flood Metadata and Floodmaps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc9a73",
   "metadata": {},
   "source": [
    "The function `filter_register_copernicusems` firsts checks that all the shapefiles follow the expected conventions with respect to timestamp and data availability. It then extracts 1) the AoI polygons, 2) hydrography information, 3) event information and 4) metadata into a separate dictionary for each ZIP file. \n",
    "\n",
    "The function `generate_floodmap` creates a geoDataFrame for each ZIP file containing the EMS-derived hydrography and flooding masks.\n",
    "\n",
    "All of this information is appended to an in-memory list variable called `registers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb805082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the event start date from the table\n",
    "code_date = table_activations_ems.loc[emsr_code][\"CodeDate\"]\n",
    "\n",
    "# Loop through the unzipped folders and extract the metadata and water masks into memory\n",
    "registers = []\n",
    "for unzip_folder in unzip_files_activations:\n",
    "    folder = os.path.split(unzip_folder)[-1]\n",
    "    \n",
    "    # Extract 1) AOI, 2) hydrography, 3) observed event into a dict\n",
    "    metadata_floodmap = activations.filter_register_copernicusems(unzip_folder, code_date)\n",
    "    \n",
    "    # Create a 'floodmap' hydrography dataframe and save to list of dictionaries called 'registers'\n",
    "    if metadata_floodmap is not None:    \n",
    "        floodmap = activations.generate_floodmap(metadata_floodmap, folder_files=unzip_folder)\n",
    "        registers.append({\"metadata_floodmap\": metadata_floodmap, \"floodmap\": floodmap})\n",
    "        print(f\"Folder {folder} processed correctly.\")\n",
    "    else:\n",
    "        print(f\"Folder {folder} does not follow the expected format. It won't be processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f9672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(\"[INFO] There are {:d} registers available.\".format(len(registers)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97296fe5",
   "metadata": {},
   "source": [
    "## Visualise the hydrology information from the first ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a535148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the metadata associated with the first ZIP file\n",
    "idx = 0\n",
    "metadata = registers[idx]['metadata_floodmap']\n",
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4abd719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the hydrology information associated with the first file\n",
    "floodmap = registers[idx]['floodmap']\n",
    "floodmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e53097d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique classes of data in the floodmap table\n",
    "np.unique(floodmap.source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca552cc8",
   "metadata": {},
   "source": [
    "These classes are:\n",
    " * area_of_interest = bounds of the area\n",
    " * flood = bounds of the flooded areas\n",
    " * hydro = lakes and rivers\n",
    " * hydro_l = linear water courses (negligible width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbba0ce1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot the flood\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "floodmap[floodmap[\"source\"] == \"area_of_interest\"].plot(ax=ax, facecolor=\"None\", edgecolor=\"yellow\",\n",
    "                                               label=\"AOI\", linewidth=0.8)\n",
    "floodmap[floodmap[\"source\"] == \"flood\"].plot(ax=ax, facecolor=\"blue\", edgecolor=\"None\", \n",
    "                                             label=\"Flood Maps\", linewidth=0.8)\n",
    "floodmap[floodmap[\"source\"] == \"hydro_l\"].plot(ax=ax, facecolor=\"None\", edgecolor=\"red\", \n",
    "                                               label=\"Hydrography Line\", linewidth=0.8)\n",
    "floodmap[floodmap[\"source\"] == \"hydro\"].plot(ax=ax, facecolor=\"magenta\", edgecolor=\"None\",\n",
    "                                               label=\"Hydrography Area\", linewidth=0.8)\n",
    "ax.set(title=\"Hydrography Map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f76beb",
   "metadata": {},
   "source": [
    "## Merge the hydrology from all ZIP files and visualise\n",
    "\n",
    "First merge the AoI and hydrology information into single dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee47ff36",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoi_list = []\n",
    "floodmap_list = []\n",
    "for register in registers:\n",
    "    # Build a list of AoI polygons\n",
    "    aoi_list.append(register[\"metadata_floodmap\"][\"area_of_interest_polygon\"])\n",
    "    # Build a list of water polygons\n",
    "    floodmap_list.append(register[\"floodmap\"])\n",
    "\n",
    "# Create a geodataframe of the AoIs and the external boundary\n",
    "crs_str = register[\"metadata_floodmap\"][\"reference system\"]\n",
    "aoi_df = gpd.GeoDataFrame(geometry=aoi_list, crs=crs_str)\n",
    "aoi_df[\"AOI_Index\"] = aoi_df.index\n",
    "aoi_ext_poly = box(*aoi_df.total_bounds)\n",
    "aoi_ext_df = gpd.GeoDataFrame(geometry=[aoi_ext_poly], crs=crs_str)\n",
    "\n",
    "# Create a dataframe of the water components\n",
    "floodmap_all = pd.concat(floodmap_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c0eea2",
   "metadata": {},
   "source": [
    "Then plot all information on a common map (may take 30 - 60 seconds).\n",
    "\n",
    "You probably don't want to do this for larger areas than shown here due to processing limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the information in the EMSR ZIP files\n",
    "fig, ax = plt.subplots(figsize=(16,16))\n",
    "floodmap_all[floodmap_all[\"source\"] == \"flood\"].plot(ax=ax, facecolor=\"blue\", edgecolor=\"None\", \n",
    "                                             label=\"Flood Maps\", linewidth=0.1)\n",
    "floodmap_all[floodmap_all[\"source\"] == \"hydro_l\"].plot(ax=ax, facecolor=\"None\", edgecolor=\"red\", \n",
    "                                               label=\"Hydrography Line\", linewidth=0.1)\n",
    "floodmap_all[floodmap_all[\"source\"] == \"hydro\"].plot(ax=ax, facecolor=\"magenta\", edgecolor=\"None\",\n",
    "                                               label=\"Hydrography Area\", linewidth=0.1)\n",
    "aoi_df.plot(ax=ax, facecolor=\"None\", edgecolor=\"black\", \n",
    "                                               label=\"AoI\", linewidth=0.1)\n",
    "aoi_ext_df.plot(ax=ax, facecolor=\"None\", edgecolor=\"black\", \n",
    "                                               label=\"AoI External\", linewidth=0.1)\n",
    "ax.set(title=\"Water Maps\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a11cb0f",
   "metadata": {},
   "source": [
    "## Determine which LGAs are affected by flooding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the LGAs from the local shapefile\n",
    "# We could also perform a query on the database\n",
    "lga_file_path = os.path.join(base_path, \"resources/LGAs/LGA_2022_AUST_GDA2020.shp\")\n",
    "lga_gdf = gpd.read_file(lga_file_path).to_crs(crs_str)\n",
    "lga_gdf.dropna(inplace=True)\n",
    "lga_gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9406b49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the unique names of the intersecting LGAs\n",
    "lga_list = lga_gdf.overlay(aoi_df, how = 'intersection')[\"LGA_NAME22\"].to_list()\n",
    "lga_list = list(set(lga_list))\n",
    "\n",
    "# Filter the LGA dataframe for the affected LGA names\n",
    "lga_affected_df = lga_gdf[lga_gdf[\"LGA_NAME22\"].isin(lga_list)]\n",
    "\n",
    "print(f\"The {len(lga_list)} affected LGAs are:\\n\", lga_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25be596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And visualise the affected LGAs\n",
    "m = lga_affected_df.explore(style_kwds={\"fillOpacity\": 0.0}, name=\"Affected LGAs\")\n",
    "aoi_df.explore(m=m, style_kwds={\"fillOpacity\": 0.1}, color=\"red\", name=\"AoIs\")\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8651b31",
   "metadata": {},
   "source": [
    "Note that the list of LGA names can be supplied as an argument to the Floodmapper data download script ```01_download_images.py```. We can format that list using a simple Python statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e51d11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a comma-separated list of LGA names\n",
    "\",\".join(lga_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addf7ab6",
   "metadata": {},
   "source": [
    "## Choose the area to map\n",
    "\n",
    "From a visual inspection of the map, we choose to map the LGAs intersected by AoIs 30 and 35, covering flooding on the Hunter river. These are: \n",
    "\n",
    "```\n",
    "['Newcastle', 'Port Stephens', 'Maitland', 'Singleton', 'Cessnock']\n",
    "```\n",
    "\n",
    "We will select these geometries and save to the local disk as a GeoJSON file for use with NEMA Floodmapper. Of course, we could also simply use all of the LGAs in ```lga_afffected_df```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1e12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the subset of LGAs to map\n",
    "lga_list = ['Newcastle', 'Port Stephens', 'Maitland', 'Singleton', 'Cessnock']\n",
    "lga_selected_df = lga_gdf[lga_gdf[\"LGA_NAME22\"].isin(lga_list)]\n",
    "lga_selected_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5ea417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the selected LGA shapes to local disk\n",
    "folder_out = os.path.join(base_path, \"flood-activations\", emsr_code).replace(\"\\\\\", \"/\")\n",
    "path_out = os.path.join(folder_out, \"LGAs_To_Map.geojson\").replace(\"\\\\\", \"/\")\n",
    "os.makedirs(folder_out, exist_ok=True)\n",
    "lga_selected_df.to_file(path_out, driver=\"GeoJSON\", crs=\"EPSG:4326\")\n",
    "print(\"[INFO] Wrote file to:\\n\", path_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4ab083",
   "metadata": {},
   "source": [
    "Alternatively, we could choose some of the EMSR AoIs to map instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b39e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write selected EMSR AoI shapes to disk\n",
    "path_out = os.path.join(folder_out, \"EMSR_AoIs_To_Map.geojson\").replace(\"\\\\\", \"/\")\n",
    "aoi_selected = aoi_df[(aoi_df.index == 30) | (aoi_df.index == 35)]\n",
    "aoi_selected.to_file(path_out, driver=\"GeoJSON\")\n",
    "print(\"[INFO] Wrote file to:\\n\", path_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444360f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the files appear on disk\n",
    "!ls {folder_out}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6ad985",
   "metadata": {},
   "source": [
    "In the next notebook, we will split these shapes into square processing patches for use with the FloodMapper system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbe742f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (floodmapper)",
   "language": "python",
   "name": "floodmapper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
