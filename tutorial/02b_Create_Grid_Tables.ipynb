{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db4664b",
   "metadata": {},
   "source": [
    "# Generate the sampling grid over Australia\n",
    "\n",
    "This notebook generates the grid of square 'patches' used for sampling areas of interest. Each patch has a unique name and is considered immutable in the FloodMapper system. This means that the patches defined here **should not be changed**. However, the grid can be expanded, provided the new patches conform to the sampling scheme (spacing and size).\n",
    "\n",
    "These geometry of these patches are stored in the database and to a file on the GCP bucket. \n",
    "\n",
    "Note: a description of the database is available in the file [documents/FloodMapper-DB_Description](documents/FloodMapper-DB_Description.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28e158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "import itertools\n",
    "from shapely.geometry import box\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from collections import defaultdict\n",
    "warnings.filterwarnings('ignore', 'Geometry is in a ', UserWarning)\n",
    "warnings.filterwarnings('ignore', '', RuntimeWarning)\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "from ml4floods.data import utils\n",
    "import fsspec\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "from db_utils import DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815f91ec",
   "metadata": {},
   "source": [
    "## Load environment and project details\n",
    "\n",
    "As with the other notebooks, we load credentials and project details from a hidden ```.env``` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cee3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (including path to credentials) from '.env' file\n",
    "env_file_path = \"../.env\"\n",
    "\n",
    "assert load_dotenv(dotenv_path=env_file_path) == True, \"[ERR] Failed to load environment!\"\n",
    "assert \"GOOGLE_APPLICATION_CREDENTIALS\" in os.environ, \"[ERR] Missing $GOOGLE_APPLICATION_CREDENTIAL!\"\n",
    "assert \"GS_USER_PROJECT\" in os.environ, \"[ERR] Missing $GS_USER_PROJECT!\"\n",
    "key_file_path = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]\n",
    "assert os.path.exists(key_file_path), f\"[ERR] Google credential key file does not exist: \\n{key_file_path} \"\n",
    "assert \"ML4FLOODS_BASE_DIR\" in os.environ, \"[ERR] Missing $ML4FLOODS_BASE_DIR!\"\n",
    "base_path = os.environ[\"ML4FLOODS_BASE_DIR\"]\n",
    "assert os.path.exists(base_path), f\"[ERR] Base path does not exist: \\n{base_path} \"\n",
    "print(\"[INFO] Successfully loaded FloodMapper environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43309ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the requester pays mode is available\n",
    "utils.check_requester_pays_gcp_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cd8a53",
   "metadata": {},
   "source": [
    "Set the details of the bucket name here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5414aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket Name - SET YOUR BUCKET NAME HERE\n",
    "bucket_name = \"gs://floodmapper-demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378b4e73",
   "metadata": {},
   "source": [
    "The remaining variables can safely be left at default values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa4f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output filename\n",
    "outfile = \"grid_australia.geojson\"\n",
    "\n",
    "# Form the path to the backup grid file on the GCP bucket\n",
    "grid_geojson_path = os.path.join(bucket_name, \"0_DEV/1_Staging/operational\", outfile)\n",
    "print(f\"[INFO] Will save master grid file to:\\n\\t{grid_geojson_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d9c812",
   "metadata": {},
   "source": [
    "## Load and visualise existing grid (if it exists)\n",
    "\n",
    "If you want to re-make the grid from scratch, set the variable `force_remake = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd60f25d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Remake the grid from scratch? (ignore any existing grid file)\n",
    "force_remake = True\n",
    "\n",
    "# Check if gridfile exists\n",
    "fs = utils.get_filesystem(grid_geojson_path)\n",
    "grid_exists = fs.exists(grid_geojson_path)\n",
    "m = 0\n",
    "\n",
    "if grid_exists and not force_remake:\n",
    "    print(\"[INFO] Existing grid found on GCP bucket - plotting.\")\n",
    "    existing_grid = utils.read_geojson_from_gcp(grid_geojson_path)\n",
    "    existing_grid = existing_grid.drop_duplicates()\n",
    "    existing_grid_full_extent = gpd.GeoDataFrame(geometry=[box(*existing_grid.total_bounds)], crs=\"EPSG:4326\")\n",
    "    m = existing_grid_full_extent.explore(style_kwds={\"fillOpacity\": 0.3,})\n",
    "    display(m)\n",
    "else:\n",
    "    grid_exists = False\n",
    "    existing_grid = None\n",
    "    existing_grid_full_extent = None\n",
    "    print(\"[INFO]: No existing grid was found on GCP (or forcing recreation).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435e96a5",
   "metadata": {},
   "source": [
    "## Extend the grid, or create from scratch\n",
    "\n",
    "We add an element to the grid if it's not in the previous grid (the intersection of the new grid element with the old grid is small)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560b780c",
   "metadata": {},
   "source": [
    "**Define the size and overlap and bounding box of the grid** - (DO NOT CHANGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc6f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid spacing and overlap\n",
    "step = 0.20, 0.20\n",
    "size = 0.21\n",
    "\n",
    "# Initial bounding box covering all of Australia\n",
    "# (long_min, lat_min, long_max, lat_max)\n",
    "bounds_initial = (112.900000000000, -44.00516044138397, \n",
    "                  153.63872785102905, -10.244936010554465)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f105afc",
   "metadata": {},
   "source": [
    "**Layout the new grid, respecting existing grid patch names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbbbf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the outline of the existing grid\n",
    "existing_grid_union = None\n",
    "grid_number = 0\n",
    "if existing_grid is not None:\n",
    "    existing_grid_union = existing_grid.geometry.unary_union\n",
    "    last_grid_name = max(existing_grid[\"name\"])\n",
    "    grid_number = int(last_grid_name.replace(\"GRID\",\"\"))\n",
    "grid_number += 1\n",
    "\n",
    "# Iterate in X and Y from min_x and min_y to generate tiles\n",
    "# bounds_initial = [min_x, min_y, max_x, max_y]\n",
    "# arange(min_x, max_x, step_x), arange(min_y, max_y, step_y)\n",
    "pols_add = []\n",
    "for x_left, y_bottom in itertools.product(np.arange(bounds_initial[0],  bounds_initial[2], step[0]), \n",
    "                                          np.arange(bounds_initial[1], bounds_initial[3], step[1])):\n",
    "    \n",
    "    # Generate patch of type Shapely box\n",
    "    bounds_iter = (x_left, y_bottom, x_left+size, y_bottom+size)\n",
    "    pol = box(*bounds_iter)\n",
    "    \n",
    "    # Skip if overlap with existing grid >= 90% overlap\n",
    "    if existing_grid is not None:\n",
    "        intersection = pol.intersection(existing_grid_union)\n",
    "        if (intersection.area / pol.area) >= 0.9:\n",
    "            continue\n",
    "        \n",
    "    # Append new grid patch to the list\n",
    "    pols_add.append({\"geometry\": pol, \"name\": f\"GRID{grid_number:05d}\"})\n",
    "    grid_number += 1\n",
    "    \n",
    "grid_add = None\n",
    "if len(pols_add) > 0:\n",
    "    grid_add = gpd.GeoDataFrame(pols_add, crs=\"EPSG:4326\")\n",
    "    print(f\"[INFO] Added {grid_add.shape[0]} grid patches.\")\n",
    "    print(grid_add.head())\n",
    "else:\n",
    "    print(f\"[INFO] No new grid patches added.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74302c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge new and existing grids\n",
    "do_upload = True\n",
    "if existing_grid is not None and grid_add is not None:\n",
    "    merged_grid = gpd.GeoDataFrame(pd.concat([existing_grid, grid_add], ignore_index=True), \n",
    "                                    crs=existing_grid.crs)\n",
    "elif existing_grid is None and grid_add is not None:\n",
    "    merged_grid = grid_add\n",
    "elif existing_grid is not None and grid_add is None:\n",
    "    merged_grid = existing_grid\n",
    "    do_upload = False\n",
    "else:\n",
    "    do_upload = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38243e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise\n",
    "merged_grid.explore(style_kwds={\"fill\":False})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba557a86",
   "metadata": {},
   "source": [
    "## Load the LGA table into the database\n",
    "\n",
    "For convenience, we want to mark each grid patch with the local government area it covers. If the patch covers more than one LGA, we make multiple entries in the 'grid_loc' table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2432f7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the latest LGA shapefile from local disk\n",
    "lga_file = os.path.join(base_path, \"resources/LGAs/LGA_2022_AUST_GDA2020.shp\")\n",
    "lga_gdf = gpd.read_file(lga_file)\n",
    "lga_gdf.dropna(inplace=True)\n",
    "lga_gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34ead0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop invalid geometries\n",
    "lga_gdf = lga_gdf[lga_gdf.geometry.is_valid]\n",
    "lga_gdf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9585a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the database (point to the .env file for credentials)\n",
    "db_conn = DB(env_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e6efb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all previous rows\n",
    "query = (f\"DELETE FROM lgas_info\")\n",
    "db_conn.run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a194dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Insert the LGA entries into the database using a batch query\n",
    "#https://naysan.ca/2020/05/09/pandas-to-postgresql-using-psycopg2-bulk-insert-performance-benchmark/\n",
    "data = []\n",
    "print(\"[INFO] Formatting rows:\")\n",
    "for row in tqdm(lga_gdf.itertuples(), total=len(lga_gdf)):\n",
    "    data.append(\n",
    "        (row.LGA_CODE22, row.LGA_NAME22, row.STE_CODE21, row.STE_NAME21,\n",
    "         row.AUS_CODE21, row.AUS_NAME21, row.AREASQKM, row.LOCI_URI21,\n",
    "         row.SHAPE_Leng, row.SHAPE_Area, str(row.geometry))\n",
    "    )\n",
    "query = (f\"INSERT INTO lgas_info\"\n",
    "         f\"(lga_code22, lga_name22, ste_code21, ste_name21, \"\n",
    "         f\"aus_code21, aus_name21, areasqkm, loci_uri21, \"\n",
    "         f\"shape_leng, shape_area, geometry_col) \"\n",
    "         f\"VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, ST_GeomFromText(%s, 25832)) \"\n",
    "         f\"ON CONFLICT (lga_name22) DO NOTHING;\")\n",
    "print(\"[INFO] Inserting table in batch mode.\", flush=True)\n",
    "db_conn.run_batch_insert(query, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851798b8",
   "metadata": {},
   "source": [
    "## Mark each grid patch with the LGA it covers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f6054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the intersection of the grid with the LGAs\n",
    "overlap_frac = 0.01\n",
    "all_rows = list()\n",
    "for name, g in tqdm(zip(lga_gdf.LGA_NAME22, lga_gdf.geometry), total = len(lga_gdf.geometry)):\n",
    "    if g:\n",
    "        aoi_idx = list()\n",
    "        for row in merged_grid.itertuples():\n",
    "            if g.intersects(row.geometry):\n",
    "                area_overlap_lga = g.intersection(row.geometry).area / row.geometry.area\n",
    "                if area_overlap_lga >= overlap_frac:\n",
    "                    aoi_idx.append(row.Index)\n",
    "        res_df = merged_grid.loc[aoi_idx]\n",
    "        res_df['LGA_NAME22'] = name\n",
    "        all_rows.extend(res_df.to_dict(orient = 'records'))\n",
    "print(\"Residual LGAs:\")\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05075f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the LGA Names into the grid table, duplicating patches at boundaries\n",
    "# Use an inner join so that we drop patches not covering a LGA\n",
    "df = pd.DataFrame(all_rows)\n",
    "all_g = pd.merge(merged_grid, df[['name', 'LGA_NAME22']], on = 'name', how = 'inner')\n",
    "final_grid_gdf = gpd.GeoDataFrame(all_g, geometry='geometry')\n",
    "final_grid_gdf = final_grid_gdf.drop_duplicates()\n",
    "final_grid_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47813770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# May need to rename and drop columns if grid file previously existed\n",
    "if grid_exists:\n",
    "    final_grid_gdf.rename({\"LGA_NAME22_x\": \"LGA_NAME22\"}, axis=1, inplace=True)\n",
    "    del final_grid_gdf[\"LGA_NAME22_y\"]\n",
    "    final_grid_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c22ddf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_grid_gdf.explore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab5c77",
   "metadata": {},
   "source": [
    "Note that the grid table now has entries for patches that overlap LGAs. This means the grid for each LGA can be selected by filtering the table by LGA Name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff822720",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the grid patches covering Cessnock LGA\n",
    "lga = \"Cessnock\"\n",
    "m = final_grid_gdf[final_grid_gdf.LGA_NAME22 == lga].explore(style_kwds={\"fillOpacity\": 0.3,})\n",
    "lga_gdf[lga_gdf.LGA_NAME22 == lga].explore(m=m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33ea078",
   "metadata": {},
   "source": [
    "## Upload new grid to GCP\n",
    "\n",
    "Upload the new grid to GCP as a GeoJSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab214869",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"[INFO] Uploading grid to: \\n\\t{grid_geojson_path}\")\n",
    "utils.write_geojson_to_gcp(grid_geojson_path, final_grid_gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59c37d",
   "metadata": {},
   "source": [
    "## Update the grid in the database.\n",
    "\n",
    "Now we run a SQL command to re-write the 'grid_loc' table in the database. This will take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f03148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all previous rows\n",
    "query = (f\"DELETE FROM grid_loc\")\n",
    "db_conn.run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063af74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the new grid entries in batch mode\n",
    "data = []\n",
    "print(\"[INFO] Formatting rows:\")\n",
    "for row in tqdm(final_grid_gdf.itertuples(), total=len(final_grid_gdf)):\n",
    "    data.append(\n",
    "        (row.name, row.LGA_NAME22, str(row.geometry))\n",
    "    )\n",
    "query = (f\"INSERT INTO grid_loc\"\n",
    "         f\"(patch_name, lga_name22, geometry) \"\n",
    "         f\"VALUES (%s, %s, ST_GeomFromText(%s, 4326)) \"\n",
    "         f\"ON CONFLICT (patch_name, lga_name22) DO NOTHING;\")\n",
    "print(\"[INFO] Inserting table in batch mode.\", flush=True)\n",
    "db_conn.run_batch_insert(query, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671bad68",
   "metadata": {},
   "source": [
    "Once the INSERT query has completed, we can check for a successful upload by querying and plotting the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea588c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and execute the query\n",
    "query = (f\"SELECT patch_name, lga_name22, ST_AsText(geometry) \"\n",
    "         f\"FROM grid_loc\")\n",
    "grid_df = db_conn.run_query(query, fetch=True)\n",
    "print(f\"[INFO] Returned {len(grid_df)} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a79aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the results into a correct GeoDataFrame\n",
    "grid_df['geometry'] = gpd.GeoSeries.from_wkt(grid_df['st_astext'])\n",
    "grid_df.drop(['st_astext'], axis=1, inplace = True)\n",
    "grid_gdf = gpd.GeoDataFrame(grid_df, geometry='geometry', crs=\"EPSG:4326\")\n",
    "grid_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b81e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot an interactive map of the grid\n",
    "grid_gdf.explore(style_kwds={\"fillOpacity\": 0.3,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4cb1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up by closing the connection to the database\n",
    "db_conn.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d7247e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (floodmapper)",
   "language": "python",
   "name": "floodmapper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
