{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75b42678",
   "metadata": {},
   "source": [
    "# Query and Visualise Available Satellite Data\n",
    "\n",
    "This notebook presents a workflow to query and visualise the Sentinel-2 and Landsat data available in Google Earth Engine under an area of interest (AOI) and over a specified time range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c223c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary Python modules\n",
    "import sys\n",
    "import os\n",
    "os.environ['USE_PYGEOS'] = '0'\n",
    "\n",
    "from ml4floods.data import utils\n",
    "import geopandas as gpd\n",
    "from georeader.readers import ee_query, scihubcopernicus_query\n",
    "import folium\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import ee\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import folium\n",
    "from zoneinfo import ZoneInfo\n",
    "import geemap.foliumap as geemap\n",
    "from georeader.readers import query_utils\n",
    "import folium\n",
    "from georeader.readers import S2_SAFE_reader\n",
    "\n",
    "# Set bucket will not be requester pays\n",
    "utils.REQUESTER_PAYS_DEFAULT = False\n",
    "\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7087918",
   "metadata": {},
   "source": [
    "## Load environment and project details\n",
    "\n",
    "As with the other notebooks, we load credentials and project details from a hidden ```.env``` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e447a084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables (including path to credentials) from '.env' file\n",
    "env_file_path = \"../.env\"\n",
    "\n",
    "# Uncomment for alternative version for Windows (r\"\" indicates raw string)\n",
    "#env_file_path = r\"C:/Users/User/floodmapper/.env\"\n",
    "\n",
    "assert load_dotenv(dotenv_path=env_file_path) == True, \"[ERR] Failed to load environment!\"\n",
    "assert \"GOOGLE_APPLICATION_CREDENTIALS\" in os.environ, \"[ERR] Missing $GOOGLE_APPLICATION_CREDENTIAL!\"\n",
    "assert \"GS_USER_PROJECT\" in os.environ, \"[ERR] Missing $GS_USER_PROJECT!\"\n",
    "key_file_path = os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]\n",
    "assert os.path.exists(key_file_path), f\"[ERR] Google credential key file does not exist: \\n{key_file_path} \"\n",
    "assert \"ML4FLOODS_BASE_DIR\" in os.environ, \"[ERR] Missing $ML4FLOODS_BASE_DIR!\"\n",
    "base_path = os.environ[\"ML4FLOODS_BASE_DIR\"]\n",
    "assert os.path.exists(base_path), f\"[ERR] Base path does not exist: \\n{base_path} \"\n",
    "bucket_name = os.environ[\"BUCKET_URI\"]\n",
    "assert bucket_name is not None and bucket_name != \"\", f\"Bucket name not defined {bucket_name}\"\n",
    "\n",
    "print(\"[INFO] Successfully loaded FloodMapper environment.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a072aec",
   "metadata": {},
   "source": [
    "**Set the details of the event and mapping session here**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48e8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All work is conducted under a unique session name\n",
    "session_name = \"EMSR570\"\n",
    "\n",
    "# Flooding date range (UTC)\n",
    "# May need to start day or two earlier\n",
    "flood_start_date = \"2022-02-23\"\n",
    "flood_end_date = \"2022-03-20\"\n",
    "\n",
    "# Pre-flood date range\n",
    "# This is a time period before the flood event to inspect reference data\n",
    "preflood_start_date = \"2022-02-10\"\n",
    "preflood_end_date = \"2022-02-20\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044edc9e",
   "metadata": {},
   "source": [
    "## Parse and check date information\n",
    "\n",
    "We assume the UTC timezone for all date queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cfa944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First parse the pre- and post-flood dates\n",
    "tz = ZoneInfo(\"UTC\")\n",
    "\n",
    "_start = datetime.strptime(flood_start_date,\"%Y-%m-%d\").replace(tzinfo=tz)\n",
    "_end = datetime.strptime(flood_end_date,\"%Y-%m-%d\").replace(tzinfo=tz)\n",
    "flood_start_period, flood_end_period = sorted([_start, _end])\n",
    "flood_duration = flood_end_period - flood_start_period\n",
    "print(f\"[INFO] Flood search period: \\n\\t{flood_start_period} to \\n\\t{flood_end_period}\")\n",
    "print(f\"[INFO] Flood duration = {flood_duration}\\n\")\n",
    "\n",
    "_start = datetime.strptime(preflood_start_date,\"%Y-%m-%d\").replace(tzinfo=tz)\n",
    "_end = datetime.strptime(preflood_end_date,\"%Y-%m-%d\").replace(tzinfo=tz)\n",
    "preflood_start_period, preflood_end_period = sorted([_start, _end])\n",
    "preflood_duration = preflood_end_period - preflood_start_period\n",
    "print(f\"[INFO] Pre-flood search period: \\n\\t{preflood_start_period} to \\n\\t{preflood_end_period}\")\n",
    "print(f\"[INFO] Pre-flood duration = {preflood_duration}\\n\")\n",
    "margin = flood_start_period - preflood_end_period\n",
    "print(f\"[INFO] Margin before flood = {margin}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2436f2",
   "metadata": {},
   "source": [
    "## Load the gridded AOIs to be mapped\n",
    "\n",
    "Here we load the gridded AoIs from the GCP bucket. We created this file in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef185b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gridded AoI filename\n",
    "grid_aoi_file = \"patches_to_map.geojson\"\n",
    "\n",
    "# Form the session path and output path on the GCP bucket\n",
    "session_path = os.path.join(bucket_name, \"0_DEV/1_Staging/operational\", session_name).replace(\"\\\\\", \"/\")\n",
    "grid_aoi_path = os.path.join(session_path, grid_aoi_file).replace(\"\\\\\", \"/\")\n",
    "grid_aois = utils.read_geojson_from_gcp(grid_aoi_path)\n",
    "grid_aois.drop_duplicates(inplace=True)\n",
    "print(f\"[INFO] Loaded gridded_aois from the following file:\\n\\t{grid_aoi_path}\")\n",
    "grid_aois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3797f827-b517-402c-a48c-090ae85bfe72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge the grid patches to form an outline (MultiPolygon)\n",
    "aoi_outline_df = grid_aois.geometry.unary_union\n",
    "aoi_outline_gdf = gpd.GeoDataFrame(geometry=[aoi_outline_df], crs=\"EPSG:4326\")\n",
    "\n",
    "# Plot the grid and outline on a Leaflet map\n",
    "m = grid_aois.explore(style_kwds={\"fillOpacity\": 0.3, \"weight\": 0.0}, name=\"Grid Patches\")\n",
    "aoi_outline_gdf.explore(m=m, color=\"red\", style_kwds={\"fillOpacity\": 0.0, \"weight\": 2.0}, \n",
    "                        name=\"AoI Outline\", highlight=False)\n",
    "folium.LayerControl(collapsed=False).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3492168-d98d-461a-bdc1-6e02807c51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any grid patches are duplicates\n",
    "are_duplicates = grid_aois.duplicated().any()\n",
    "print(f\"Are any grid names duplicates? -> {are_duplicates}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f342705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any duplicates now\n",
    "grid_aois.drop_duplicates(inplace=True)\n",
    "grid_aois.duplicated().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e30343-ae97-40c3-be24-0eaab44ce69a",
   "metadata": {},
   "source": [
    "## Query what images are available in Google Earth Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faba3ad-6e3e-4208-8c5c-a8fec7411696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the Google Earth Engine connection.\n",
    "# Follow instructions on login prompt, if required.\n",
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb3726-05a1-4ea2-9a94-bdc62bffc140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the GEE project\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f98a8-01ec-4c7c-8a8b-776b03f5308a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Run a GEE query for Landsat and Sentinel-2 data.\n",
    "#  producttype can be 'both', 'S2', \"Landsat\", \"L8\" or \"L9\".\n",
    "#  add_s2cloudless adds a column that indicates if the s2cloudless image is available .\n",
    "flood_images_gee, flood_collection = ee_query.query(\n",
    "    area=aoi_outline_df, \n",
    "    date_start=flood_start_period, \n",
    "    date_end=flood_end_period,                                                   \n",
    "    producttype=\"both\", \n",
    "    return_collection=True, \n",
    "    add_s2cloudless=True)\n",
    "\n",
    "# Print data about the available images\n",
    "num_images = flood_images_gee.shape[0]\n",
    "print(f\"[INFO] Found {num_images} flooding images on archive.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87c7b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same query for the pre-flood period\n",
    "preflood_images_gee, preflood_collection = ee_query.query(\n",
    "    area=aoi_outline_df, \n",
    "    date_start=preflood_start_period, \n",
    "    date_end=preflood_end_period,                                                   \n",
    "    producttype=\"both\", \n",
    "    return_collection=True, \n",
    "    add_s2cloudless=True)\n",
    "num_images = preflood_images_gee.shape[0]\n",
    "print(f\"[INFO] Found {num_images} pre-flood images on archive.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff2b6c9-0210-4e7a-ae42-a00063e91393",
   "metadata": {},
   "source": [
    "## Visualise the available Landsat and S2 imagery\n",
    "\n",
    "Here we can directly visualise the imagery for each satellite overpass. This will help make a selection on which days to include in the flood mapping operation.\n",
    "\n",
    "Once the map loads, click on individual Satellite + Date combinations to show the imagery. \n",
    "\n",
    "**Note: the map and imagery can take a few seconds to load.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa231b1-bc16-4f9b-9eef-91ccbf52f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Intialise the OpenStreetMap base layer\n",
    "m = geemap.Map(location=aoi_outline_df.centroid.coords[0][-1::-1], zoom_start=8)\n",
    "\n",
    "# Add the pre-flood data\n",
    "for (day, satellite), images_day in preflood_images_gee.groupby([\"solarday\", \"satellite\"]):    \n",
    "    image_col_day_sat = preflood_collection.filter(ee.Filter.inList(\"title\", images_day.index.tolist()))    \n",
    "    bands = [\"B11\",\"B8\",\"B4\"] if satellite.startswith(\"S2\") else [\"B6\",\"B5\",\"B4\"] \n",
    "    m.addLayer(image_col_day_sat, \n",
    "               {\"min\":0, \"max\":3000 if satellite.startswith(\"S2\") else 0.3, \"bands\": bands},\n",
    "               f\"{satellite}: {day}\",\n",
    "               False)\n",
    "    \n",
    "# Add the flooding data\n",
    "for (day, satellite), images_day in flood_images_gee.groupby([\"solarday\", \"satellite\"]):    \n",
    "    image_col_day_sat = flood_collection.filter(ee.Filter.inList(\"title\", images_day.index.tolist()))    \n",
    "    bands = [\"B11\",\"B8\",\"B4\"] if satellite.startswith(\"S2\") else [\"B6\",\"B5\",\"B4\"]\n",
    "    m.addLayer(image_col_day_sat, \n",
    "               {\"min\":0, \"max\":3000 if satellite.startswith(\"S2\") else 0.3, \"bands\": bands},\n",
    "               f\"{satellite}: {day}\",\n",
    "               False)\n",
    "\n",
    "aoi_outline_gdf.explore(style_kwds={\"fillOpacity\": 0.1}, color=\"black\", name=\"AoI\", highlight=False, m=m)\n",
    "folium.LayerControl(collapsed=False).add_to(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb894ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the map\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a33f3d9",
   "metadata": {},
   "source": [
    "For the EMSR570 flood, we can see the following days provide good imagery:\n",
    "\n",
    "**Reference Period:**\n",
    " * S2A: 2022-02-11\n",
    " * LC08: 2022-02-12\n",
    " * LC09: 2022-02-13\n",
    " * S2A: 2022-02-18\n",
    " * S2B: 2022-02-19\n",
    " \n",
    "**Flood Period:**\n",
    " * LC08: 2022-02-28\n",
    " * S2B: 2022-03-01\n",
    " * LC08: 2022-03-02\n",
    " * LC09: 2022-03-03\n",
    " * S2A: 2022-03-03\n",
    " * LC08: 2022-03-04\n",
    " * S2B: 2022-03-05\n",
    " * S2A: 2022-03-06\n",
    " * LC09: 2022-03-08\n",
    " * S2B: 2022-03-09\n",
    " * LC09: 2022-03-10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (floodmapper)",
   "language": "python",
   "name": "floodmapper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
